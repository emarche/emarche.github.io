<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Enrico Marchesini </title> <meta name="author" content="Enrico Marchesini"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="reinforcement learning, safety, multi-agent systems, evolutionary algorithms, safety, robotics, power systems, computer science, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?a91f7317c8e9ec8b28b7164213674575"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emarche.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" align="center"> <span class="font-weight-bold">Enrico</span> Marchesini </h1> <p class="desc" align="center"><strong>Postdoctoral</strong> Research Associate<br></p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 950px) 276.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpeg?47a5bee7c6192e25829b16f10a9cc165" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>MIT 45-601k</p> </div> </div> <div class="clearfix"> <p>I am a Postdoctoral Associate at <a href="https://www.mit.edu" rel="external nofollow noopener" target="_blank">Massachusetts Institute of Technology</a> advised by <a href="https://priyadonti.com/" rel="external nofollow noopener" target="_blank">Prof. Priya Donti</a>, in the <a href="https://lids.mit.edu" rel="external nofollow noopener" target="_blank">Laboratory for Information &amp; Decision Systems</a>. Previously I was a Postdoctoral Associate with <a href="https://www.khoury.northeastern.edu/people/chris-amato/" rel="external nofollow noopener" target="_blank">Prof. Christopher Amato</a> at <a href="https://www.northeastern.edu" rel="external nofollow noopener" target="_blank">Northeastern University</a>, and a Ph.D. student at the University of Verona, with <a href="http://profs.sci.univr.it/~farinelli" rel="external nofollow noopener" target="_blank">Prof. Alessandro Farinelli</a>.</p> <p>My <strong>research interests</strong> are driven by the impact that reinforcement learning (RL) could have in the real-world, where effective <em>exploration</em>, <em>safety</em>, and <em>asynchronous execution</em> are key requirements for autonomous agents. Hence, I focus on developing deep RL algorithms aimed at tackling these fundamental challenges in simulation, <em>multi-agent</em> systems, and realistic applications. I am currently working on safe and asynchronous multi-agent RL and will focus on its applications to power systems during my Postdoc.</p> <p>Outside of work, I spend time climbing and hiking.</p> </div> <hr> <div class="news"> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">2025</th> <td>  <strong>January</strong> <ul> <li>Our works <a href="https://arxiv.org/abs/2408.15381" rel="external nofollow noopener" target="_blank">“On Stateful Value Factorization in Multi-Agent Reinforcement Learning”</a> and <a href="https://arxiv.org/abs/2406.08315" rel="external nofollow noopener" target="_blank">“Improving Policy Optimization via ε-Retrain”</a> have been accepted at AAMAS 2025.</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td>  <strong>January</strong> <ul> <li>Our paper “Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees” has been accepted at <a href="https://aaai.org/aaai-conference/" rel="external nofollow noopener" target="_blank">AAAI 2024</a>. Congratulations to <a href="https://lmarza.github.io" rel="external nofollow noopener" target="_blank">Luca Marzari</a> for leading the project! What a great start to 2024!</li> <li>Another collaboration with the colleagues from <a href="https://oregonstate.edu" rel="external nofollow noopener" target="_blank">Oregon State University</a>, <a href="https://www.linkedin.com/in/alp-aydeniz" rel="external nofollow noopener" target="_blank">Alp Aydeniz</a>, and <a href="https://web.engr.oregonstate.edu/~ktumer/" rel="external nofollow noopener" target="_blank">Kagan Tumer</a> has been accepted at AAMAS 2024. Check out our abstract “Entropy Seeking Constrained Multiagent Reinforcement Learning” during the conference in May!</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td>  <strong>September</strong> <ul> <li>Excited to share our first collaboration with <a href="https://www.linkedin.com/in/alp-aydeniz" rel="external nofollow noopener" target="_blank">Alp Aydeniz</a>, <a href="https://www.robert-loftin.net" rel="external nofollow noopener" target="_blank">Robert Loftin</a>, and <a href="https://web.engr.oregonstate.edu/~ktumer/" rel="external nofollow noopener" target="_blank">Kagan Tumer</a> has been accepted at <a href="https://sites.bu.edu/mrs2023/" rel="external nofollow noopener" target="_blank">MRS 2023</a>. Check out <em>“Entropy Maximization in High Dimensional Multiagent State Spaces”</em> during the conference in December!</li> </ul>  <strong>May</strong> <ul> <li>I’m thrilled to announce my new adventure as a Postdoctoral researcher at <a href="https://www.mit.edu" rel="external nofollow noopener" target="_blank">MIT</a>; more details will come in November! Working with <a href="https://www.khoury.northeastern.edu/people/chris-amato/" rel="external nofollow noopener" target="_blank">Chris Amato</a> has been great and I’m looking forward to continuing our collaborations!</li> </ul>  <strong>January</strong> <ul> <li>Our paper <em><a href="/assets/publication_news/ICLR2023_vfs">“Improving Deep Policy Gradients with Value Function Search”</a></em> has been accepted at <a href="https://iclr.cc" rel="external nofollow noopener" target="_blank">ICLR 2023</a>.</li> <li>Our paper <em><a href="/assets/publication_news/AAMAS2023_violationpenalty">“Safe Deep Reinforcement Learning by Verifying Task-Level Properties”</a></em> has been accepted at <a href="https://aamas2023.soton.ac.uk" rel="external nofollow noopener" target="_blank">AAMAS 2023</a>.</li> <li>Our paper <em><a href="/assets/publication_news/ICRA2023_crop">“Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation”</a></em> has been accepted at <a href="https://www.icra2023.org" rel="external nofollow noopener" target="_blank">ICRA 2023</a>.</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td>  <strong>July</strong> <ul> <li>Presenting “<em>Safety-Informed Mutations for Evolutionary Deep Reinforcement Learning</em>” at <a href="https://sites.google.com/view/evorl/home" rel="external nofollow noopener" target="_blank">Gecco EvoRL Workshop</a>.</li> </ul>  <strong>April</strong> <ul> <li>Excited to join <a href="https://www.khoury.northeastern.edu/people/chris-amato/" rel="external nofollow noopener" target="_blank">Prof. Christopher Amato</a> as a Postdoctoral researcher at <a href="https://www.northeastern.edu" rel="external nofollow noopener" target="_blank">Northeastern University</a>.</li> </ul>  <strong>January</strong> <ul> <li>Our paper “<em>Enhancing Deep Reinforcement Learning Approaches for Multi-Robot Navigation via Single-Robot Evolutionary Policy Search</em>” has been accepted at <a href="https://www.icra2022.org" rel="external nofollow noopener" target="_blank">ICRA 2022</a>.</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td>  <strong>November</strong> <ul> <li>Our paper “<em>Exploring Safer Behaviors for Deep Reinforcement Learning</em>” has been accepted at <a href="https://aaai-2022.virtualchair.net" rel="external nofollow noopener" target="_blank">AAAI 2022</a> <strong>(15% acceptance rate)</strong>.</li> </ul>  <strong>June</strong> <ul> <li> <strong>Three</strong> papers have been accepted at <a href="https://ieeexplore.ieee.org/xpl/conhome/9635848/proceeding" rel="external nofollow noopener" target="_blank">IROS 2021</a>: <ul> <li>“<em>Centralizing State-Values in Dueling Networks for Multi-Robot Reinforcement Learning Mapless Navigation</em>.”</li> <li>“<em>Benchmarking Safe Deep Reinforcement Learning in Aquatic Navigation</em>.”</li> <li>“<em>Safe Reinforcement Learning using Formal Verification for Tissue Retraction in Autonomous Robotic-Assisted Surgery</em>.”</li> </ul> </li> </ul> </td> </tr> </table> </div> </div> <hr> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAMAS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AAMAS2025_statefulfact-480.webp 480w,/assets/img/publication_preview/AAMAS2025_statefulfact-800.webp 800w,/assets/img/publication_preview/AAMAS2025_statefulfact-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/AAMAS2025_statefulfact.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="AAMAS2025_statefulfact.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="statefuligm" class="col-sm-8"> <div class="title">On Stateful Value Factorization in Multi-Agent Reinforcement Learning</div> <div class="author"> <em>Enrico Marchesini</em>, Andrea Baisero, Rupali Bhati, and Christopher Amato </div> <div class="periodical"> <em>In (to appear) International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html//assets/publication_news/AAMAS2025_statefulfact" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Value factorization is a popular paradigm for designing scalable multi-agent reinforcement learning algorithms. However, current factorization methods make choices without full justification that may limit their performance. For example, the theory in prior work uses stateless (i.e., history) functions, while the practical implementations use state information – making the motivating theory a mismatch for the implementation. Also, methods have built off of previous approaches, inheriting their architectures without exploring other, potentially better ones. To address these concerns, we formally analyze the theory of using the state instead of the history in current methods – reconnecting theory and practice. We then introduce DuelMIX, a factorization algorithm that learns distinct per-agent utility estimators to improve performance and achieve full expressiveness. Experiments on StarCraft II micromanagement and Box Pushing tasks demonstrate the benefits of our intuitions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">statefuligm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Stateful Value Factorization in Multi-Agent Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marchesini, Enrico and Baisero, Andrea and Bhati, Rupali and Amato, Christopher}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{(to appear) International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2408.15381}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAMAS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AAMAS2025_epsretrain-480.webp 480w,/assets/img/publication_preview/AAMAS2025_epsretrain-800.webp 800w,/assets/img/publication_preview/AAMAS2025_epsretrain-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/AAMAS2025_epsretrain.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="AAMAS2025_epsretrain.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="epsretrain" class="col-sm-8"> <div class="title">Improving Policy Optimization via ε-Retrain</div> <div class="author"> Luca Marzari, Changliu Liu, Priya Donti, and <em>Enrico Marchesini</em> </div> <div class="periodical"> <em>In (to appear) International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html//assets/publication_news/AAMAS2025_epsretrain" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>We present ε-retrain, an exploration strategy designed to encourage a behavioral preference while optimizing policies with monotonic improvement guarantees. To this end, we introduce an iterative procedure for collecting retrain areas – parts of the state space where an agent did not follow the behavioral preference. Our method then switches between the typical uniform restart state distribution and the retrain areas using a decaying factor ε, allowing agents to retrain on situations where they violated the preference. Experiments over hundreds of seeds across locomotion, navigation, and power network tasks show that our method yields agents that exhibit significant performance and sample efficiency improvements. Moreover, we employ formal verification of neural networks to provably quantify the degree to which agents adhere to behavioral preferences.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">epsretrain</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Policy Optimization via \epsilon-Retrain}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marzari, Luca and Liu, Changliu and Donti, Priya and Marchesini, Enrico}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{(to appear) International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.08315}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ICLR2023_vfs-480.webp 480w,/assets/img/publication_preview/ICLR2023_vfs-800.webp 800w,/assets/img/publication_preview/ICLR2023_vfs-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/ICLR2023_vfs.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ICLR2023_vfs.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ICLR2023_vfs" class="col-sm-8"> <div class="title">Improving Deep Policy Gradients via Value Function Search</div> <div class="author"> <em>Enrico Marchesini</em>, and Christopher Amato </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html//assets/publication_news/ICLR2023_vfs" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Deep Policy Gradient (PG) algorithms employ value networks to drive the learning of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. In this paper, we focus on improving value approximation and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the supervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ICLR2023_vfs</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Deep Policy Gradients via Value Function Search}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marchesini, Enrico and Amato, Christopher}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=6qZC7pfenQm}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <hr> <div class="social"> <div class="contact-icons"> <a href="mailto:%65%6D%61%72%63%68%65@%6D%69%74.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/enrico-marchesini-b25342190" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=9V1_SGkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/_emarche" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">Feel free to send me an email to chat about RL! </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Enrico Marchesini. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 19, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>